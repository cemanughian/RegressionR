{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "November 3 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we had \n",
    "\n",
    "linear regression for contunious output data \n",
    "\n",
    "$Y = X\\beta + \\epsilon$\n",
    "\n",
    "Logistic for 0,1, output data\n",
    "\n",
    "Log(P(Yi=1)/1-P(Yi=1) = XB\n",
    "\n",
    "Now we have\n",
    "Poisson regression for count variables\n",
    "we still want to model \n",
    "\n",
    "LOG(E[Y]) = XB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLM\n",
    "\n",
    "F(E[y]) = XB\n",
    "\n",
    "model a transformation of the expected value as a linear combination of covariates\n",
    "the q is how to figure out what transformation is best\n",
    "in LinReg there is no trans\n",
    "in LogReg it was the log of the ratio of the expected value over 1-expected value. this elaborate transformaiton was needed to make sure the LHS and RHS were on the same range from -inf to inf. with the logit we stretched it to inf which is usually where linear transformation of covariates resides.\n",
    "\n",
    "However we cant count events ina negative direciton, so we want to make sure our transformation doesnt allow my model to predict negative counts. \n",
    "\n",
    "We want to take a transformation of an RV, a count. What would be reasonable?\n",
    "Whats the expected value of a count variable, like a poisson RV? Positive number, doesnt have to be an interger. What transformation of somehting strictly positive do we have to implement to stretch it to +/- infinity. \n",
    "\n",
    "Log should do it. Thats the first realization. \n",
    "\n",
    "LOG(E[Y]) = XB\n",
    "\n",
    "Does it make sense to fix the time period of which we are observing the events of interest? SMoking 10 cigs today is not the same as 10 in ur lifetime. (Also called Log-Linear regression)\n",
    "\n",
    "Modeling the events of interests over a specific period of time. \n",
    "\n",
    "In real life, people are observed over different periods of time. You cant compare the number of occurances of seizures over a year to that of a month. SO there is a way in Preg to correct for the fact that people are observed over diff time periods, called offset.\n",
    "\n",
    "offset is a specific variable that you dont calculate the effect of  but you force it into the model to account for the fact that people are observed over different periods of time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.5.20\n",
    "Remember last time we said generalized linear models used a \n",
    "transformation on the expected value of the outcome variable and model that as a linear combination of the covariates:\n",
    "\n",
    "\n",
    "g(E[y]) = XB\n",
    "\n",
    "If y is continuous:\n",
    "Linear:\n",
    "g(E[y]) = E[y]\n",
    "\n",
    "If y is bernoulli (binomial)\n",
    "Y ~BERN(p) = BIN(1,p)\n",
    "\n",
    "Logistic REgression:\n",
    "g(E[y])= LOG(E[y]/(1-E[y]))\n",
    "\n",
    "If Y is a count variable or rate:\n",
    "\n",
    "Poisson (or \"log-linear\") regression:\n",
    "g(E[y]) = LOG(E[y]) \n",
    "\n",
    "What are the dangers of modeling count data with a forced linear regression? Count data is positive and intergers and linreg lin combo of covariates can go to -inf to inf, so its likely that your model can start predicting negative counts, which is not good. The other idea is, can you take the log of the count, and then do linear regression. People do this, but what could be a problem with that? The likelyhood will be normal likelyhood which doesnt respect integer values, its a continuous distribution, which immediately makes me uneasy when were modeling count data which is very specific ineger outcome and youre adopting a distribution thta is contunious for estimation, so there is a mismatch on the type of outcome variable where one is discrete and one is continuous. You can have issues with possible negative values and predicting non integer values as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just see what is going on herE:\n",
    "\n",
    "Remember these are all conditional expectations here:\n",
    "${log}(E[y_i | x_i]) = X_i \\beta $\n",
    "\n",
    "                    ith subjects covariates,*common effect sizes\n",
    "Funcitons that connect the E[y] to the LC of covars is called cannonical functions. Other transformations are possible. \n",
    "\n",
    "We can exponentiate both sides:\n",
    "\n",
    "$(E[y_i | x_i]) = e^{X_i \\beta} $\n",
    "call the lhs $\\mu_i$\n",
    "$ \\mu_i = e^{X_i \\beta} $\n",
    "\n",
    "This is random sample from a poisson distribution with a common mean\n",
    "$Y_i, ... Y_n ~ Poisson(\\mu)$\n",
    "\n",
    "The ith person has a poisson distribution with a subject specific mean \n",
    "\n",
    "$ \\mu_i = e^{X_i \\beta} $ \n",
    "\n",
    "where $ X_i \\beta$ are iid, they have subject specific means. \n",
    "\n",
    "$e^{any  power}$ is positive so this seems like a reasonable model. Generalization of a random sample with common contruction of the means, they share the effect sizes betas but allow differences dictated by the subject specific covariate bodies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "There are several questions here to answer:\n",
    "* How does R estimate these betas?\n",
    "\n",
    "We want to estimate betas and their covariance matrix. From there we can test hypothesis and build models. \n",
    "\n",
    "R Constructs the likelyhood of the betas:\n",
    "\n",
    "$L(\\beta) = \\product_{i=1, n} e^{-\\mu_i} / y_i! , \\mu_i = e^{X_i\\beta}$\n",
    "\n",
    "We can plug it in:\n",
    "\n",
    "(18 min)\n",
    "\n",
    "$L(\\beta) = \\product_{i=1, n} e^{e^{-\\mu_i}} (e^{X_i \\beta})^{y_i} / y_i! , \\mu_i = e^{X_i\\beta}$\n",
    "\n",
    "R maximizes this with iterative methods to find beta_hat, the maximum likelyhood.\n",
    "\n",
    "Observed information matrix\n",
    "\n",
    "Bhat T = (B1 .. bhatk) is the information matrix:\n",
    "\n",
    "(k+1)x(k+1)\n",
    "\n",
    "whats the relationship of the information matrix and the var covar matrix of the regression coefs?\n",
    "it is the inverse of the information matrix. \n",
    "\n",
    "Cov(bhat) = I-1(Bhat)\n",
    "\n",
    "from here you also estimate the variances on the main diagonal of the covariance matrix to test hypothesis of signficiance of the variables. R does this behind the scenes. \n",
    "**\n",
    "\n",
    "_____\n",
    "We want to know what the regression coefs mean\n",
    "\n",
    "Interpretation of poisson regression coefs:\n",
    "\n",
    "Let $Y =$ number of accidents on a highway\n",
    "\n",
    "So\n",
    "$\\log(E[Y]) = \\beta_0 + \\beta_1*age$\n",
    "\n",
    "age is how old the road is, single variable continuous\n",
    "\n",
    "We fit the model, get beta0hat and beta1hat\n",
    "and coefs are stat sig. What are the interpretations of  \\beta_0 hatand  \\beta_1hat ?\n",
    "\n",
    "Remember $E[Y] = e^{\\beta_{hat_0}}$\n",
    "This is the expected number of accidents for road of age 0 . \n",
    "\n",
    "What is the inrepretation of the beta1 coef?\n",
    "Lets grab 1 road of age a, and a second road of age a+1\n",
    "\n",
    "expected accidents for road 2, E[Y_2], would be \n",
    "\n",
    "$E[Y_2]  = e^{\\beta_{hat_0} + \\beta_{hat1}*(a+1)}$\n",
    "$E[Y_1]  = e^{\\beta_{hat_0} + \\beta_{hat1}*a}$\n",
    "\n",
    "Now we take the ratio:\n",
    "$E[Y_2]/E[Y_1]  = e^{\\beta_{1hat}}$\n",
    "\n",
    "So the intepretaiton of the slope of beta1 would be whta:\n",
    "it will give me the increase or decrease of odds attributable to 1 unit increase in odds of the variable.\n",
    "\n",
    "when we exponentiate a \n",
    "(30)\n",
    "<b>% increase or decrease in the expected counts attributable to 1 unit increase of the predictor, so 1 unit increase in age. \n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what would happen if we have a categorical variable?\n",
    "\n",
    "\n",
    "Interpretation of poisson regression coefs for categorical input:\n",
    "\n",
    "Let $Y =$ number of accidents on a highway.\n",
    "\n",
    "X = type of road , rural city or highway\n",
    "\n",
    "So say rural is baseline, then\n",
    "\n",
    "$\\log(E[Y]) = \\beta_0 + \\beta_1*I_{city} + \\beta_2 *I_{highway}$\n",
    "\n",
    "We get the hats estimates with their standard errors and signficiance and we want to find out what these betas are telling us:\n",
    "\n",
    "$e^{\\beta_{0hat}} = E[Y]$\n",
    "\n",
    "expected number of accidents for the baseline group which is rural roads. \n",
    "\n",
    "What about $\\beta_1$?\n",
    "Well\n",
    "\n",
    "$e^{\\beta_{0hat} + \\beta_{1hat}}$\n",
    "would be the expected number of accidents for city roads.\n",
    "now we can take the ratio\n",
    "\n",
    "$e^{\\beta_{0hat}} / e^{\\beta_{0hat} + \\beta_{1hat}} = e^{\\beta_{1hat}}$\n",
    "\n",
    "ratio of expected accidents for city /expected accidents for rural\n",
    "\n",
    "so when we exponentate poisson assoc w level of categorical variable, the excess number of occurence of events of interest compared to baseline. percentage increase in expected counts for this group compared to the baseline.\n",
    "\n",
    "Similar for beta2hat:\n",
    "\n",
    "$e^{\\beta_{0hat} + \\beta_{2hat}}$\n",
    "would be the expected number of accidents for highway roads.\n",
    "now we can take the ratio\n",
    "\n",
    "$e^{\\beta_{0hat}} / e^{\\beta_{0hat} + \\beta_{2hat}} = e^{\\beta_{2hat}}$\n",
    "\n",
    "\n",
    "expected number of accidents on highway / exp number of accidents rural\n",
    "we can see how many more accidents, percentage wise, we expect to see on a highway comapred to rural by exponentiating beta2hat. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets grab a data set and see if we can build a model, get R to fit it. \n",
    "\n",
    "Problem with dispersion, and different subjects being followed for different amounts of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(foreign) # dta dataset\n",
    "df = read.dta(\"needle_sharing.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 3 × 17</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>id</th><th scope=col>sex</th><th scope=col>ethn</th><th scope=col>age</th><th scope=col>dprsn_dx</th><th scope=col>sexabuse</th><th scope=col>shared_syr</th><th scope=col>hivstat</th><th scope=col>hplsns</th><th scope=col>nivdu</th><th scope=col>shsyryn</th><th scope=col>sqrtnivd</th><th scope=col>logshsyr</th><th scope=col>polydrug</th><th scope=col>sqrtninj</th><th scope=col>homeless</th><th scope=col>shsyr</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>2104</td><td>M</td><td>White</td><td>47</td><td>5</td><td>0</td><td>1</td><td>0</td><td> 6</td><td>90</td><td>1</td><td>9.486833</td><td>0</td><td>0</td><td>9.486833</td><td>0</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2009</td><td>M</td><td>White</td><td>39</td><td>1</td><td>0</td><td>1</td><td>1</td><td> 2</td><td> 4</td><td>1</td><td>2.000000</td><td>0</td><td>0</td><td>2.000000</td><td>1</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>2032</td><td>M</td><td>White</td><td>52</td><td>1</td><td>0</td><td>1</td><td>0</td><td>18</td><td>90</td><td>1</td><td>9.486833</td><td>0</td><td>0</td><td>9.486833</td><td>0</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 3 × 17\n",
       "\\begin{tabular}{r|lllllllllllllllll}\n",
       "  & id & sex & ethn & age & dprsn\\_dx & sexabuse & shared\\_syr & hivstat & hplsns & nivdu & shsyryn & sqrtnivd & logshsyr & polydrug & sqrtninj & homeless & shsyr\\\\\n",
       "  & <int> & <chr> & <chr> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 2104 & M & White & 47 & 5 & 0 & 1 & 0 &  6 & 90 & 1 & 9.486833 & 0 & 0 & 9.486833 & 0 & 1\\\\\n",
       "\t2 & 2009 & M & White & 39 & 1 & 0 & 1 & 1 &  2 &  4 & 1 & 2.000000 & 0 & 0 & 2.000000 & 1 & 1\\\\\n",
       "\t3 & 2032 & M & White & 52 & 1 & 0 & 1 & 0 & 18 & 90 & 1 & 9.486833 & 0 & 0 & 9.486833 & 0 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 3 × 17\n",
       "\n",
       "| <!--/--> | id &lt;int&gt; | sex &lt;chr&gt; | ethn &lt;chr&gt; | age &lt;int&gt; | dprsn_dx &lt;int&gt; | sexabuse &lt;int&gt; | shared_syr &lt;int&gt; | hivstat &lt;int&gt; | hplsns &lt;int&gt; | nivdu &lt;int&gt; | shsyryn &lt;dbl&gt; | sqrtnivd &lt;dbl&gt; | logshsyr &lt;dbl&gt; | polydrug &lt;dbl&gt; | sqrtninj &lt;dbl&gt; | homeless &lt;dbl&gt; | shsyr &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 2104 | M | White | 47 | 5 | 0 | 1 | 0 |  6 | 90 | 1 | 9.486833 | 0 | 0 | 9.486833 | 0 | 1 |\n",
       "| 2 | 2009 | M | White | 39 | 1 | 0 | 1 | 1 |  2 |  4 | 1 | 2.000000 | 0 | 0 | 2.000000 | 1 | 1 |\n",
       "| 3 | 2032 | M | White | 52 | 1 | 0 | 1 | 0 | 18 | 90 | 1 | 9.486833 | 0 | 0 | 9.486833 | 0 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  id   sex ethn  age dprsn_dx sexabuse shared_syr hivstat hplsns nivdu shsyryn\n",
       "1 2104 M   White 47  5        0        1          0        6     90    1      \n",
       "2 2009 M   White 39  1        0        1          1        2      4    1      \n",
       "3 2032 M   White 52  1        0        1          0       18     90    1      \n",
       "  sqrtnivd logshsyr polydrug sqrtninj homeless shsyr\n",
       "1 9.486833 0        0        9.486833 0        1    \n",
       "2 2.000000 0        0        2.000000 1        1    \n",
       "3 9.486833 0        0        9.486833 0        1    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>128</li><li>17</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 128\n",
       "\\item 17\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 128\n",
       "2. 17\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 128  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(df)\n",
    "# 128 people w 17 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id           sex                ethn                age       \n",
       " Min.   :2001   Length:128         Length:128         Min.   :19.00  \n",
       " 1st Qu.:2034   Class :character   Class :character   1st Qu.:35.00  \n",
       " Median :2066   Mode  :character   Mode  :character   Median :41.00  \n",
       " Mean   :2065                                         Mean   :40.73  \n",
       " 3rd Qu.:2097                                         3rd Qu.:47.25  \n",
       " Max.   :2129                                         Max.   :54.00  \n",
       "                                                                     \n",
       "    dprsn_dx        sexabuse        shared_syr        hivstat     \n",
       " Min.   :1.000   Min.   :0.0000   Min.   : 0.000   Min.   :0.000  \n",
       " 1st Qu.:1.000   1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.000  \n",
       " Median :1.000   Median :0.0000   Median : 0.000   Median :0.000  \n",
       " Mean   :2.206   Mean   :0.1371   Mean   : 2.976   Mean   :0.114  \n",
       " 3rd Qu.:5.000   3rd Qu.:0.0000   3rd Qu.: 0.000   3rd Qu.:0.000  \n",
       " Max.   :5.000   Max.   :1.0000   Max.   :60.000   Max.   :2.000  \n",
       " NA's   :2       NA's   :4        NA's   :5        NA's   :14     \n",
       "     hplsns           nivdu          shsyryn        sqrtnivd     \n",
       " Min.   : 0.000   Min.   :  0.0   Min.   :0.00   Min.   : 0.000  \n",
       " 1st Qu.: 2.000   1st Qu.: 42.5   1st Qu.:0.00   1st Qu.: 6.516  \n",
       " Median : 5.000   Median : 90.0   Median :0.00   Median : 9.487  \n",
       " Mean   : 6.609   Mean   :101.9   Mean   :0.25   Mean   : 8.982  \n",
       " 3rd Qu.:10.000   3rd Qu.:120.0   3rd Qu.:0.25   3rd Qu.:10.954  \n",
       " Max.   :20.000   Max.   :900.0   Max.   :1.00   Max.   :30.000  \n",
       "                  NA's   :1                      NA's   :1       \n",
       "    logshsyr         polydrug         sqrtninj         homeless     \n",
       " Min.   :0.0000   Min.   :0.0000   Min.   : 0.000   Min.   :0.0000  \n",
       " 1st Qu.:0.6932   1st Qu.:0.0000   1st Qu.: 6.516   1st Qu.:0.0000  \n",
       " Median :1.6094   Median :0.0000   Median : 9.487   Median :0.0000  \n",
       " Mean   :1.7765   Mean   :0.1484   Mean   : 8.982   Mean   :0.4919  \n",
       " 3rd Qu.:2.3026   3rd Qu.:0.0000   3rd Qu.:10.954   3rd Qu.:1.0000  \n",
       " Max.   :4.0943   Max.   :1.0000   Max.   :30.000   Max.   :1.0000  \n",
       " NA's   :101                       NA's   :1        NA's   :4       \n",
       "     shsyr      \n",
       " Min.   : 1.00  \n",
       " 1st Qu.: 2.00  \n",
       " Median : 5.00  \n",
       " Mean   :13.56  \n",
       " 3rd Qu.:10.00  \n",
       " Max.   :60.00  \n",
       " NA's   :101    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(df) # See the summary of our variables missingness, mistypes, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     sex                ethn                age           dprsn_dx    \n",
       " Length:128         Length:128         Min.   :19.00   Min.   :1.000  \n",
       " Class :character   Class :character   1st Qu.:35.00   1st Qu.:1.000  \n",
       " Mode  :character   Mode  :character   Median :41.00   Median :1.000  \n",
       "                                       Mean   :40.73   Mean   :2.206  \n",
       "                                       3rd Qu.:47.25   3rd Qu.:5.000  \n",
       "                                       Max.   :54.00   Max.   :5.000  \n",
       "                                                       NA's   :2      \n",
       "    sexabuse        shared_syr        hivstat          hplsns      \n",
       " Min.   :0.0000   Min.   : 0.000   Min.   :0.000   Min.   : 0.000  \n",
       " 1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.000   1st Qu.: 2.000  \n",
       " Median :0.0000   Median : 0.000   Median :0.000   Median : 5.000  \n",
       " Mean   :0.1371   Mean   : 2.976   Mean   :0.114   Mean   : 6.609  \n",
       " 3rd Qu.:0.0000   3rd Qu.: 0.000   3rd Qu.:0.000   3rd Qu.:10.000  \n",
       " Max.   :1.0000   Max.   :60.000   Max.   :2.000   Max.   :20.000  \n",
       " NA's   :4        NA's   :5        NA's   :14                      \n",
       "    shsyryn        polydrug         homeless     \n",
       " Min.   :0.00   Min.   :0.0000   Min.   :0.0000  \n",
       " 1st Qu.:0.00   1st Qu.:0.0000   1st Qu.:0.0000  \n",
       " Median :0.00   Median :0.0000   Median :0.0000  \n",
       " Mean   :0.25   Mean   :0.1484   Mean   :0.4919  \n",
       " 3rd Qu.:0.25   3rd Qu.:0.0000   3rd Qu.:1.0000  \n",
       " Max.   :1.00   Max.   :1.0000   Max.   :1.0000  \n",
       "                                 NA's   :4       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Is char type for ethnicity a good type? No, it should be a factor.\n",
    "# Sex, ethnicity, should be factor\n",
    "# # We see missingness in hivstat, whcih is like 10% and out dataset isnt that big\n",
    "# In the other variables, not too many values are missing. \n",
    "# we might get away with excluding rows with missing data. \n",
    "# Why do we have a lot of missing data on shared syringes?\n",
    "# What variables should we remove? \n",
    "# id, we also have variations of the same variable\n",
    "df2 = df[,-c(1,10,12,13,15,17)] # todo - how to subset with names instead of int\n",
    "\n",
    "summary(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too much missingness except HIV status\n",
    "Might still be some redundancy. \n",
    "hivstat should be an indicator variable w 0 or 1, and we can group the 2s into the 1 since theres only 2 samples w 2. \n",
    "\n",
    "\n",
    "What kind of count variables can we explore?\n",
    "\n",
    "number of drugs or number of shared syringes are the two count variable choices. \n",
    "\n",
    "hes proposing to model the number of shared syringes. what makes people share syringes when they know disease are spread instantaneously thru needles?\n",
    "(61 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2$sex <- as.factor(df2$sex)\n",
    "df2$ethn <- as.factor(df2$ethn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    sex           ethn         age           dprsn_dx        sexabuse     \n",
       " F    :30   White   :76   Min.   :19.00   Min.   :1.000   Min.   :0.0000  \n",
       " M    :97   AA      :36   1st Qu.:35.00   1st Qu.:1.000   1st Qu.:0.0000  \n",
       " Trans: 1   Hispanic:10   Median :41.00   Median :1.000   Median :0.0000  \n",
       "            Filipino: 2   Mean   :40.73   Mean   :2.206   Mean   :0.1371  \n",
       "            Asian   : 1   3rd Qu.:47.25   3rd Qu.:5.000   3rd Qu.:0.0000  \n",
       "            Indian  : 1   Max.   :54.00   Max.   :5.000   Max.   :1.0000  \n",
       "            (Other) : 2                   NA's   :2       NA's   :4       \n",
       "   shared_syr        hivstat          hplsns          shsyryn    \n",
       " Min.   : 0.000   Min.   :0.000   Min.   : 0.000   Min.   :0.00  \n",
       " 1st Qu.: 0.000   1st Qu.:0.000   1st Qu.: 2.000   1st Qu.:0.00  \n",
       " Median : 0.000   Median :0.000   Median : 5.000   Median :0.00  \n",
       " Mean   : 2.976   Mean   :0.114   Mean   : 6.609   Mean   :0.25  \n",
       " 3rd Qu.: 0.000   3rd Qu.:0.000   3rd Qu.:10.000   3rd Qu.:0.25  \n",
       " Max.   :60.000   Max.   :2.000   Max.   :20.000   Max.   :1.00  \n",
       " NA's   :5        NA's   :14                                     \n",
       "    polydrug         homeless     \n",
       " Min.   :0.0000   Min.   :0.0000  \n",
       " 1st Qu.:0.0000   1st Qu.:0.0000  \n",
       " Median :0.0000   Median :0.0000  \n",
       " Mean   :0.1484   Mean   :0.4919  \n",
       " 3rd Qu.:0.0000   3rd Qu.:1.0000  \n",
       " Max.   :1.0000   Max.   :1.0000  \n",
       "                  NA's   :4       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the rare ethnicities into 1 group, possibly combining even with hispanic. \"other\"\n",
    "\n",
    "Also remove every row with missing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2$ethn <- df2[df2$ethn ] # Group the ethnicity variable\n",
    "# \"0-5\"==data[data$x>0 & data$x<5, ]\n",
    "\n",
    "# Drop rows with any NAs in them \n",
    "df2 <- df2[rowSums(is.na(df2)) == 0, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    sex                 ethn         age           dprsn_dx    \n",
       " F    :27   White         :67   Min.   :19.00   Min.   :1.000  \n",
       " M    :82   AA            :31   1st Qu.:35.25   1st Qu.:1.000  \n",
       " Trans: 1   Hispanic      : 7   Median :41.00   Median :1.000  \n",
       "            Filipino      : 2   Mean   :40.79   Mean   :1.982  \n",
       "            Indian        : 1   3rd Qu.:47.75   3rd Qu.:1.000  \n",
       "            Indian & White: 1   Max.   :54.00   Max.   :5.000  \n",
       "            (Other)       : 1                                  \n",
       "    sexabuse        shared_syr        hivstat           hplsns      \n",
       " Min.   :0.0000   Min.   : 0.000   Min.   :0.0000   Min.   : 0.000  \n",
       " 1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.: 2.000  \n",
       " Median :0.0000   Median : 0.000   Median :0.0000   Median : 5.000  \n",
       " Mean   :0.1182   Mean   : 2.873   Mean   :0.1182   Mean   : 6.491  \n",
       " 3rd Qu.:0.0000   3rd Qu.: 0.000   3rd Qu.:0.0000   3rd Qu.:10.000  \n",
       " Max.   :1.0000   Max.   :60.000   Max.   :2.0000   Max.   :20.000  \n",
       "                                                                    \n",
       "    shsyryn          polydrug         homeless     \n",
       " Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n",
       " 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n",
       " Median :0.0000   Median :0.0000   Median :1.0000  \n",
       " Mean   :0.2091   Mean   :0.1727   Mean   :0.5091  \n",
       " 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n",
       " Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n",
       "                                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = shared_syr ~ homeless, family = poisson, data = df2)\n",
       "\n",
       "Deviance Residuals: \n",
       "   Min      1Q  Median      3Q     Max  \n",
       "-3.111  -3.111  -1.291  -1.291  13.849  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)  -0.1823     0.1491  -1.223    0.221    \n",
       "homeless      1.7591     0.1610  10.928   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for poisson family taken to be 1)\n",
       "\n",
       "    Null deviance: 1482.5  on 109  degrees of freedom\n",
       "Residual deviance: 1311.2  on 108  degrees of freedom\n",
       "AIC: 1397.9\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(glm(shared_syr~homeless, data=df2, family=poisson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember you exponentiate the coef\n",
    "ratio of expected numbered of syringed shared\n",
    "\n",
    "number of expected shared syringes is 5.9 times higher atributable to homlessness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "5.91268474593706"
      ],
      "text/latex": [
       "5.91268474593706"
      ],
      "text/markdown": [
       "5.91268474593706"
      ],
      "text/plain": [
       "[1] 5.912685"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp(1.7771) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the next best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = shared_syr ~ hplsns, family = poisson, data = df2)\n",
       "\n",
       "Deviance Residuals: \n",
       "   Min      1Q  Median      3Q     Max  \n",
       "-2.579  -2.518  -2.401  -2.080  15.642  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)  1.20137    0.08292   14.49   <2e-16 ***\n",
       "hplsns      -0.02386    0.01056   -2.26   0.0238 *  \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for poisson family taken to be 1)\n",
       "\n",
       "    Null deviance: 1482.5  on 109  degrees of freedom\n",
       "Residual deviance: 1477.1  on 108  degrees of freedom\n",
       "AIC: 1563.8\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(glm(shared_syr~hplsns, data=df2, family=poisson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.979385445943952"
      ],
      "text/latex": [
       "0.979385445943952"
      ],
      "text/markdown": [
       "0.979385445943952"
      ],
      "text/plain": [
       "[1] 0.9793854"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp(-0.02083)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each point higher on the helplessness score the expected number of shared needles drops by 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = shared_syr ~ sex, family = poisson, data = df2)\n",
       "\n",
       "Deviance Residuals: \n",
       "   Min      1Q  Median      3Q     Max  \n",
       "-2.816  -2.323  -2.323  -2.323  16.049  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)   1.37764    0.09325  14.774  < 2e-16 ***\n",
       "sexM         -0.38478    0.11260  -3.417 0.000633 ***\n",
       "sexTrans    -13.68022  284.65918  -0.048 0.961670    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for poisson family taken to be 1)\n",
       "\n",
       "    Null deviance: 1633.9  on 122  degrees of freedom\n",
       "Residual deviance: 1616.9  on 120  degrees of freedom\n",
       "  (5 observations deleted due to missingness)\n",
       "AIC: 1721.9\n",
       "\n",
       "Number of Fisher Scoring iterations: 10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(glm(shared_syr~sex, data=df2, family=poisson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.905878579619867"
      ],
      "text/latex": [
       "0.905878579619867"
      ],
      "text/markdown": [
       "0.905878579619867"
      ],
      "text/plain": [
       "[1] 0.9058786"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp(-.09885)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline female\n",
    "nothing is significant here. if there was a significant, \n",
    "what would be the effect for being male against female?\n",
    "\n",
    "being male is 30% less expected count of shared needles compared to female\n",
    "\n",
    "EXPECTED NUMBER OF SHARED NEEDLES FOR MALES IS 10% LOWER THAN THE EXPECTED (AVG) NUMBER OF SHARED NEEDLES FOR FEMALES . IF IT WERE A SIGNIFICANT VARIABLE IT WOULD BE A GOOD THING, since its decreasing something bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.10.20 \n",
    "\n",
    "### How people, after they build their poisson reg, how they figure out if the model fits the data well\n",
    "\n",
    "We built our best model, but dontk now if its a good model. \n",
    "\n",
    "In LinReg we have AIC, BIC, r2, adj-r2, residual diagnostics. All the discrepancies and misspecification of the model is shown in the behavior of the residuals. \n",
    "\n",
    "This weekend: Organize notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model, find the AUROC, find the optimal threshold, maybe f1 score. All these guys can tell us if the model fits the data well. If you can afford it you can split the data into train/val. \n",
    "\n",
    "How do we defend that the best model we found is a good model? Theres a specific tool for poisson regression models called the <b>deviance</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Deviance for assess Poisson regression performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y_i$ ~ Poisson($\\mu_i$), $\\mu_i = e^{\\beta_0 + \\beta_1 x1 + ... + \\beta_p x_ip$\n",
    "\n",
    "poisson regression imposes the above on the mean.\n",
    "\n",
    "after we estimate the betas, we can estimate the subject-specific mean, or the count, for the event of interest.\n",
    "\n",
    "\n",
    "We can define the deviance statistic for the model:\n",
    "\n",
    "$ D = -2* sum(i=1,n) [y_i log(\\mu_{i, hat} ) - \\mu_{i,hat} - log(y_i !)] - 2 sum(i=1, n) [(y_i)(log(y_i) - y_i log(y_i !) ]\n",
    "$\n",
    "\n",
    "compares the (profile) likelihood of the model we build against the likelyhood of the saturated model. The saturated model makes a perfect prediction for everybody. \n",
    "\n",
    "How close to perfect in terms of likelihood is ur model compared to perfect model?\n",
    "\n",
    "Well D ~ chi^2 (n-p)\n",
    "n-p df\n",
    "where n is the smaple size and p is number of covariates in model\n",
    "\n",
    "sample size is number of variables in the saturated model and p is the number of variables in our model.\n",
    "\n",
    "So what is the null hypothesis? That our model is good\n",
    "\n",
    "$H_0$ : our model is good\n",
    "\n",
    "$H_a$ : our model is not good\n",
    "\n",
    "if we reject and get a small p value, the model, which was the best we could find, is not good enough. \n",
    "\n",
    "dispersion parameter needs to be relaxed, which makes the likelihood quasi poisson. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.12.20 More on the Deviance Statistic\n",
    "\n",
    "R outputs 2 deviances. The one we really want is called the residual deviance. We will just call this the deviance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we constructed the likelihood of the data and it was  bunch of poissons:\n",
    "\n",
    "L(B) = product(i=1, mu) e^-mu_i  * mu_i ^ y /yi! => Bhat_MLE\n",
    "\n",
    "where ui = e ...\n",
    "so ui is fed into LB and maximized\n",
    "\n",
    "So we can calculate\n",
    "LOG(L(Bhat)) to get maximum value of the log likelihood by plugging in the max likelihood value with the betas\n",
    "\n",
    "If we multiply this by -2 well call this deviance:\n",
    "\n",
    "#### Deviance of the model:\n",
    "\n",
    "-2*LOG(L(Bhat))\n",
    "\n",
    "\n",
    "Do we want a model with small or large deviance? Small deviance cuz we want a big log likelihood.\n",
    "\n",
    "Q: Why are we doing the log of the likelihood? We usually take the log of the likelihood and dont work with the likelihood cuz it has better properties, like the sum breaks the product above into sums and sums of things behave a lot better than produce. Its a simplification not a complication, to make thing much better behaved.\n",
    "\n",
    "Likelihood theory, LRT always take the log of the likelihood so the distribution becomes CHI2.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theres also something that R reports called the \n",
    "\n",
    "#### Null Deviance\n",
    "Deviance of a model with just an intercept (simplest possible model)\n",
    "\n",
    "On the other side of the spectrum is the deviance for the saturated model:\n",
    "\n",
    "#### Saturated Deviance \n",
    "\n",
    "A model that has as many variables as observations, which ensures perfect prediction. \n",
    "\n",
    "\n",
    "Q: What would be the ML values of all these subject-specific parameters? Remember every subject will have mu_i which is the mODEL PREDICTED PROBABILTY OF THE COUNT FOR THE ITH PERSON. everyone has their own so it will be the observed value, yi: mu_hat_i = y_i. Very hard to use such a model but ensures perfect prediction on the val set. \n",
    "\n",
    "Based on this, we define the \n",
    "\n",
    "### Residual Deviance\n",
    "\n",
    "This is the differences in the deviances of our model and the saturated model.\n",
    "\n",
    "We will choose as our parameters, those that result in the greatest likelihood computed. The estimates are called maximum likelihood because the parameters are chosen to maximize the likelihood (conditional probability of the data given parameter estimates) of the sample data. The techniques actually employed to find the maximum likelihood estimates fall under the general label numerical analysis.\n",
    "\n",
    "\n",
    "D = -2* sum(i=1,n) [yi LOG(mu_i_hat) - mu_i_hat - LOG(y_i!)] \n",
    "    -2* sum(i=1,n) [yi LOG(y_i) - y_i - LOG(y_i!)] \n",
    "Becuase in the second one the max likelihood is yi itself. \n",
    "\n",
    "so using properties of logs we get:\n",
    "D = -2* sum(i=1,n) [yi LOG(yi/mu_i_hat) - yi - mu_i_hat] \n",
    "\n",
    "### We can use the Residual Deviance to see if the model is good\n",
    "We can use the residual deviance as a goodness of fit statistic.\n",
    "\n",
    "What do we need to know? We have to know the dsitribution of the residual deviance to estimate how small the difference between deviances of our model and saturated model is.\n",
    "\n",
    "D ~ X^2(n-(p+1))\n",
    "total number parms in our model - total number parms in saturated model\n",
    "\n",
    "This measure comes from R. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other notes on Poisson Distribution. \n",
    "\n",
    "Some issues arise with Poisson.\n",
    "\n",
    "Remember if we have an RV X ~ Poi(mu) we know that\n",
    "E[x] = mu and var(x) = mu\n",
    "\n",
    "in normal the mean ad variance are unrelated things. in poisson the mean and var exactly the same, which is sometimes a problem, because the <b>variance in the real life data is bigger than the mean</b>. There is more variablity than the poisson distribution mandates. \n",
    "\n",
    "In fact, if you fit a poisson regression, there is a line that says \"over dispersion parameter is estimated at 1\".\n",
    "\n",
    "This ensures that the mean and the variance are the same. In real world data, you see a lot more variability than the linear model.\n",
    "\n",
    "people do two things:\n",
    "they relax the distribution that you impose on the data to \"Quasi Poisson\"\n",
    "\n",
    "You still keep the regression as is but allows dispersion parameter to be greater than 1, 5,8,10.\n",
    "\n",
    "Allows the model to fit the data better at the expense of using something that is not a proper distribution. If you allow the variance to be bigger than the mean, it is no longer a proper poisson distribution. This is like a relaxed guideline of a Poisson distribution. Allow the variance to be a multiple of the mean: 5,8,10...etc...\n",
    "\n",
    "When the dispersion parameter=1, the mean*1=variance. Relaxing this allows variance=dispersionparam*mean.\n",
    "\n",
    "Downside? No longer a poisson distribution likelihood. Its not a real likelihood so you dont get an AIC value so the beautiful minimizing AIC doesnt work any more so at least you get a better fit by minimizing p values. \n",
    "(23)\n",
    "\n",
    "Sometimes people move away from Likelihood to generalized estimate equations to better fit data that doesnt match the traditional distributions.You need to explore the metadata to see if it produces biased results. Youll find if you fit a qpoisson model you dont get biased estimates. (todo)\n",
    "\n",
    "Negative binomial regression.\n",
    "\n",
    "There is also a staistical tets that tells you if you should go into overdispersion models. If you are wondering when you should do it. \n",
    "\n",
    "Quasi poisson - you can still maximize it to get the qpois mle values. they happen to be the same as mle pois. the aic is the penalized version of the likelihood so when you dont have  likelihood you dont have the aic. so if you want to build automated models you cant used stepAIC.\n",
    "\n",
    "SEcond approach is: Negative binomial regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Bionmial Regression\n",
    "\n",
    "Math is a Bayesian Argument (todo)\n",
    "\n",
    "These guys allow us to have a mean \n",
    "E[Yhat] = muhat = e^(X_i^T*mu)\n",
    "But the variance has a overdispersion parameter which allows the variance to be bigger by being a mixture of two poissons:\n",
    "VARhat[Y] = muhat + mu_i^2/theta\n",
    "\n",
    "Allows the vairance to be bigger than the mean. \n",
    "(29 min)\n",
    "\n",
    "<b>\n",
    "Overdispersion is a common problem with count data. 2 ways to handle: relax poisson distribution to qpoisson which changes the standard errors but not MLE from before. Negative bionomial distribution changes everything, still based on poisson but a mixture of poissons. Allows the variance to be bigger while still imposing a structured parameters on the data. Interpretation and parametrs is the same, but they are fitted in distribution and different estimate equations. \n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with poisson\n",
    "test for overdispersion (if the test doesnt fail, stick with poisson) (if there is overdispersion explore both other options: qpoisson and negative binomial. probably choose nbin models cuz its based on a proper distribution, which inherits all the properties and the interpretation of parameters doesnt change so we dont have to completely change our mindest and interpretation scheme.)\n",
    "\n",
    "it is rare for poisson to fit the data with no overdispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember Negative Binomial Distribution\n",
    "\n",
    "<strike>Models the number of successes before a failure (wait no thats geometric). </strike>\n",
    "\n",
    "Discerete probaility distribution.\n",
    "\n",
    "Models the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures (denoted r) occurs.\n",
    "\n",
    "Imagine we have a series of independent and identical trials. Each trial can result in a success or failure (drilling for oil). \n",
    "(38 min)\n",
    "p(success) = p\n",
    "p(failure) = 1-p\n",
    "\n",
    "X = # of trials until the rth success.\n",
    "X~NB(r,p)\n",
    "\n",
    "x can be: r, r+1, r+2, ...\n",
    "f(x), probability that we exactly x successes. f(z) = P(X=x) (number of trials until rth success): p^(r-1)\n",
    "\n",
    "Remember we have r successes each occuring with probability p, and we have x-r failures. in how many waits can we sprinkle r-1 sucessses amoung the first x-1 trials: \n",
    "(x-1 C r-1) *p^r(1-p)^(x-r)\n",
    "We can move the successes and failures in that many ways and still have that many successes and failures. (42 min)**** review\n",
    "\n",
    "Is this related to modeling count data? What is the PDF?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math behind it real quick:\n",
    "\n",
    "\n",
    "Y | lambda ~ Pois(lambda)\n",
    "on the lambda, we impose a prior distribution of \n",
    "\n",
    "lambda| mu, theta   ~ Gamma(theta, theta/mu)\n",
    "\n",
    "Multi level distribution of a model. y depends on lambda but lambda depnds on other params. \n",
    "\n",
    "Dis is classical structures where we have poisson distribution of data and gamma distribution for the parameters. \n",
    "\n",
    "f(lambda | mu, theta) = theta/mu)^thera / gamma(theta)   * lambda^(theta-1) * e^-lambda*theta/mu\n",
    "\n",
    "means: f(y|lambda) = lambda^y * e^-lambda / y!\n",
    "\n",
    "Integrating out the intermediate variable lamdba out of the picture:\n",
    "\n",
    "f(y| mu, theta) integral(-inf, inf) f(y| lambda) f(lambda|mu, theta)\n",
    "\n",
    "Then you get\n",
    "\n",
    "(gamma(y+theta) /gamma(theta)y!) (theta/(theta+mu))^theta * (1- (theta/(theta+mu))^y\n",
    "\n",
    "if you look closely this is a negtive binomial:\n",
    "()*p^0(1-p)^0\n",
    "if you calculate the expected value of y, the undoncitional ones,\n",
    "u will get what we got in the begining and we allow for overdispersion.\n",
    "This is not the simplest derivation. We just wanted to see that its not the most obvious thing. In fact very few people know about negative binomial regression, what it is, why its better, how to fit it in R. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out Poisson variations with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
